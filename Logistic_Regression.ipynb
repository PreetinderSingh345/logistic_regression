{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0970ece05f2fd367a433285c0204e778ad1644066d163bed046b3b0abfdd35b59",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed libraries/modules/packages\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection, preprocessing as PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the breast cancer dataset and getting the input, output and the feature names\n",
    "\n",
    "data=datasets.load_breast_cancer()\n",
    "\n",
    "X=data.data\n",
    "Y=data.target\n",
    "\n",
    "columns=data.feature_names\n",
    "N=len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst radius  \\\n",
       "count     569.000000              569.000000  ...    569.000000   \n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       worst texture  worst perimeter   worst area  worst smoothness  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       worst compactness  worst concavity  worst concave points  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       worst symmetry  worst fractal dimension  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>0.181162</td>\n      <td>0.062798</td>\n      <td>...</td>\n      <td>16.269190</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>0.027414</td>\n      <td>0.007060</td>\n      <td>...</td>\n      <td>4.833242</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.106000</td>\n      <td>0.049960</td>\n      <td>...</td>\n      <td>7.930000</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>0.161900</td>\n      <td>0.057700</td>\n      <td>...</td>\n      <td>13.010000</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>0.179200</td>\n      <td>0.061540</td>\n      <td>...</td>\n      <td>14.970000</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>0.195700</td>\n      <td>0.066120</td>\n      <td>...</td>\n      <td>18.790000</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>0.304000</td>\n      <td>0.097440</td>\n      <td>...</td>\n      <td>36.040000</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# describing the input data through a dataframe\n",
    "\n",
    "df=pd.DataFrame(X, columns=columns)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data, and we're standardising it due to the presence of outliers as it will make the calculations fast and will remove the dominance of the features with large scale over those with small scale\n",
    "\n",
    "scaler=PP.StandardScaler()\n",
    "\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "X=np.concatenate((X, np.ones(X.shape[0]).reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing, getting numpy arrays for the training part and concatinating a column with all 1s  to the last\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=model_selection.train_test_split(X, Y, random_state=1)\n",
    "\n",
    "M=X_train.shape[0]\n",
    "\n",
    "np_x=np.array(X_train)\n",
    "np_y=np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score function which predicts the mean average score or the accuracy for the predictions made\n",
    "\n",
    "def score(Y_pred, Y_true):\n",
    "    count_right_predictions=0\n",
    "    total_predictions=len(Y_pred)\n",
    "\n",
    "    for i in range(len(Y_pred)):        \n",
    "        if Y_pred[i]==Y_true[i]:\n",
    "            count_right_predictions+=1\n",
    "\n",
    "    score=(count_right_predictions/total_predictions)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the type of breast cancer as malignant or benign depending on the feature values, using the optimal coefficients calculated through gradient descent \n",
    "\n",
    "def predict(x, coeffs):\n",
    "    y_pred=np.zeros(x.shape[0])\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        mtxi=(coeffs*x[i]).sum()\n",
    "        hxi=1/(1+math.exp(-mtxi))\n",
    "\n",
    "        y_pred[i]=hxi\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step gradient function which makes a change in the value of the coefficients, so that we move closer towards the optimal cost \n",
    "\n",
    "def step_gradient(coeffs, learning_rate):\n",
    "    slope=np.zeros(N+1)    \n",
    "\n",
    "    for j in range(N+1):\n",
    "        for i in range(M):\n",
    "            yi=np_y[i]\n",
    "            mtxi=(coeffs*np_x[i]).sum()\n",
    "            hxi=1/(1+math.exp(-mtxi))\n",
    "            xij=np_x[i][j]\n",
    "\n",
    "            slope[j]+=(xij*(yi-hxi))\n",
    "\n",
    "    slope/=(-M)\n",
    "    coeffs-=(learning_rate*slope)\n",
    "\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function which calculates the cost/error for a particular value of the coefficients \n",
    "\n",
    "def cost(coeffs):  \n",
    "    cost=0\n",
    "     \n",
    "    for i in range(M):\n",
    "        yi=np_y[i]        \n",
    "        mtxi=(coeffs*np_x[i]).sum()\n",
    "\n",
    "        ei=(yi*mtxi)-math.log(1+math.exp(mtxi))\n",
    "\n",
    "        cost+=ei\n",
    "\n",
    "    cost/=(-M)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd function which performs the gradient descent and returns the optimal coefficients for which the cost is minimised\n",
    "\n",
    "def gd(learning_rate):    \n",
    "    coeffs=np.zeros(N+1)\n",
    "\n",
    "    prev_cost=cost(coeffs)\n",
    "\n",
    "    # choosing the value of learning rate or aplha for which we just don't overshoot and the cost starts decreasing\n",
    "\n",
    "    while True:                \n",
    "        new_coeffs=step_gradient(coeffs, learning_rate)\n",
    "        new_cost=cost(new_coeffs)    \n",
    "\n",
    "        if new_cost>=prev_cost:\n",
    "            learning_rate/=10                 \n",
    "        else:\n",
    "            break\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while True:   \n",
    "        prev_cost=cost(coeffs)\n",
    "\n",
    "        coeffs=step_gradient(coeffs, learning_rate)\n",
    "        new_cost=cost(coeffs)\n",
    "\n",
    "        print(\"Cost\", i, new_cost)\n",
    "\n",
    "        # when the difference between the new and prev cost is <= 0.0001, then we simply break and return the optimal coefficients \n",
    "\n",
    "        if abs(new_cost-prev_cost)<=0.0001:\n",
    "            break\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    return coeffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function which runs the gradient descent code\n",
    "\n",
    "def run():\n",
    "    # initial value of learning rate or alpha\n",
    "    \n",
    "    learning_rate=0.1\n",
    "\n",
    "    coeffs=gd(learning_rate)\n",
    "\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost 0 0.42624090990666574\n",
      "Cost 1 0.3719832908404567\n",
      "Cost 2 0.3348206054307278\n",
      "Cost 3 0.30741806679756645\n",
      "Cost 4 0.28617363691709097\n",
      "Cost 5 0.26909556230910586\n",
      "Cost 6 0.25498572215655624\n",
      "Cost 7 0.24307646906945968\n",
      "Cost 8 0.23285118467683255\n",
      "Cost 9 0.2239481977200138\n",
      "Cost 10 0.21610596863283316\n",
      "Cost 11 0.20913016428533712\n",
      "Cost 12 0.2028730182678479\n",
      "Cost 13 0.1972199179346669\n",
      "Cost 14 0.19208041505449802\n",
      "Cost 15 0.18738203833948452\n",
      "Cost 16 0.1830659341015939\n",
      "Cost 17 0.17908373111393008\n",
      "Cost 18 0.17539524430182865\n",
      "Cost 19 0.17196676506075712\n",
      "Cost 20 0.16876976939150398\n",
      "Cost 21 0.1657799285567015\n",
      "Cost 22 0.16297634206644387\n",
      "Cost 23 0.16034093629102103\n",
      "Cost 24 0.15785798800424253\n",
      "Cost 25 0.15551374324714137\n",
      "Cost 26 0.15329610969690893\n",
      "Cost 27 0.15119440628247777\n",
      "Cost 28 0.14919915779924076\n",
      "Cost 29 0.14730192520451685\n",
      "Cost 30 0.14549516443750313\n",
      "Cost 31 0.14377210821935796\n",
      "Cost 32 0.14212666650205352\n",
      "Cost 33 0.14055334215545798\n",
      "Cost 34 0.13904715918696456\n",
      "Cost 35 0.13760360133179012\n",
      "Cost 36 0.13621855927479332\n",
      "Cost 37 0.13488828509562478\n",
      "Cost 38 0.1336093527899651\n",
      "Cost 39 0.13237862392666275\n",
      "Cost 40 0.131193217665991\n",
      "Cost 41 0.13005048449711956\n",
      "Cost 42 0.12894798316032985\n",
      "Cost 43 0.12788346030680497\n",
      "Cost 44 0.12685483252018148\n",
      "Cost 45 0.12586017038265113\n",
      "Cost 46 0.12489768431678432\n",
      "Cost 47 0.12396571197435838\n",
      "Cost 48 0.12306270697689206\n",
      "Cost 49 0.12218722884054896\n",
      "Cost 50 0.12133793394154106\n",
      "Cost 51 0.1205135673979694\n",
      "Cost 52 0.11971295576078528\n",
      "Cost 53 0.11893500042078371\n",
      "Cost 54 0.11817867165065869\n",
      "Cost 55 0.11744300321151029\n",
      "Cost 56 0.11672708746207382\n",
      "Cost 57 0.11603007091657397\n",
      "Cost 58 0.1153511502036931\n",
      "Cost 59 0.11468956838482725\n",
      "Cost 60 0.11404461159473954\n",
      "Cost 61 0.11341560597199879\n",
      "Cost 62 0.11280191485032277\n",
      "Cost 63 0.11220293618519937\n",
      "Cost 64 0.11161810019300804\n",
      "Cost 65 0.11104686718234913\n",
      "Cost 66 0.11048872555949125\n",
      "Cost 67 0.10994318999176442\n",
      "Cost 68 0.1094097997144282\n",
      "Cost 69 0.10888811696804634\n",
      "Cost 70 0.1083777255547207\n",
      "Cost 71 0.10787822950272012\n",
      "Cost 72 0.1073892518300765\n",
      "Cost 73 0.10691043339865454\n",
      "Cost 74 0.10644143185101981\n",
      "Cost 75 0.10598192062317714\n",
      "Cost 76 0.10553158802689663\n",
      "Cost 77 0.10509013639594414\n",
      "Cost 78 0.10465728129105174\n",
      "Cost 79 0.10423275075894235\n",
      "Cost 80 0.10381628464114148\n",
      "Cost 81 0.1034076339286944\n",
      "Cost 82 0.10300656015925008\n",
      "Cost 83 0.1026128348532795\n",
      "Cost 84 0.10222623898648135\n",
      "Cost 85 0.10184656249567274\n",
      "Cost 86 0.10147360381570016\n",
      "Cost 87 0.10110716944510227\n",
      "Cost 88 0.10074707353845401\n",
      "Cost 89 0.10039313752348321\n",
      "Cost 90 0.10004518974121013\n",
      "Cost 91 0.09970306510750032\n",
      "Cost 92 0.09936660479454743\n",
      "Cost 93 0.09903565593091973\n",
      "Cost 94 0.09871007131891432\n",
      "Cost 95 0.09838970916805202\n",
      "Cost 96 0.09807443284364406\n",
      "Cost 97 0.09776411062943444\n",
      "Cost 98 0.0974586155034035\n",
      "Cost 99 0.09715782492587961\n",
      "Cost 100 0.09686162063917414\n",
      "Cost 101 0.09656988847800788\n",
      "Cost 102 0.09628251819005262\n",
      "Cost 103 0.09599940326595817\n",
      "Cost 104 0.09572044077827976\n",
      "Cost 105 0.09544553122876433\n",
      "Cost 106 0.09517457840348696\n",
      "Cost 107 0.094907489235371\n",
      "Cost 108 0.09464417367364725\n",
      "Cost 109 0.09438454455984835\n",
      "Cost 110 0.0941285175099555\n",
      "Cost 111 0.09387601080233846\n",
      "Cost 112 0.09362694527116115\n",
      "Cost 113 0.09338124420493762\n",
      "Cost 114 0.09313883324994976\n",
      "Cost 115 0.09289964031825464\n",
      "Cost 116 0.09266359550002624\n",
      "Cost 117 0.0924306309799931\n",
      "Cost 118 0.09220068095774939\n",
      "Cost 119 0.0919736815717276\n",
      "Cost 120 0.09174957082663805\n",
      "Cost 121 0.09152828852419016\n",
      "Cost 122 0.09130977619692143\n",
      "Cost 123 0.09109397704497144\n",
      "Cost 124 0.09088083587564784\n",
      "Cost 125 0.09067029904564047\n",
      "Cost 126 0.09046231440574672\n",
      "Cost 127 0.09025683124798216\n",
      "Cost 128 0.09005380025495498\n",
      "Cost 129 0.08985317345139134\n",
      "Cost 130 0.08965490415770405\n",
      "Cost 131 0.08945894694550507\n",
      "Cost 132 0.08926525759496609\n",
      "Cost 133 0.08907379305393572\n",
      "Cost 134 0.08888451139873213\n",
      "Cost 135 0.08869737179652756\n",
      "Cost 136 0.08851233446925004\n",
      "Cost 137 0.08832936065893215\n",
      "Cost 138 0.08814841259443572\n",
      "Cost 139 0.08796945345949306\n",
      "Cost 140 0.08779244736199832\n",
      "Cost 141 0.08761735930449667\n",
      "Cost 142 0.08744415515581339\n",
      "Cost 143 0.08727280162377296\n",
      "Cost 144 0.08710326622895782\n",
      "Cost 145 0.08693551727946192\n",
      "Cost 146 0.08676952384659302\n",
      "Cost 147 0.08660525574148432\n",
      "Cost 148 0.08644268349257354\n",
      "Cost 149 0.0862817783239111\n",
      "Cost 150 0.08612251213426635\n",
      "Cost 151 0.08596485747699052\n",
      "Cost 152 0.08580878754061008\n",
      "Cost 153 0.08565427613011647\n",
      "Cost 154 0.08550129764892335\n",
      "Cost 155 0.08534982708146459\n",
      "Cost 156 0.0851998399764047\n",
      "Cost 157 0.0850513124304378\n",
      "Cost 158 0.08490422107264994\n",
      "Cost 159 0.08475854304942262\n",
      "Cost 160 0.08461425600985484\n",
      "Cost 161 0.08447133809168382\n",
      "Cost 162 0.08432976790768201\n",
      "Cost 163 0.08418952453251557\n",
      "Cost 164 0.08405058749004161\n",
      "Cost 165 0.0839129367410302\n",
      "Cost 166 0.08377655267129183\n",
      "Cost 167 0.08364141608019793\n",
      "Cost 168 0.08350750816957592\n",
      "Cost 169 0.08337481053296603\n",
      "Cost 170 0.08324330514522649\n",
      "Cost 171 0.08311297435247246\n",
      "Cost 172 0.08298380086233728\n",
      "Cost 173 0.08285576773454355\n",
      "Cost 174 0.08272885837177227\n",
      "Cost 175 0.08260305651081952\n",
      "Cost 176 0.08247834621402868\n",
      "Cost 177 0.08235471186099087\n",
      "Cost 178 0.08223213814050022\n",
      "Cost 179 0.08211061004275795\n",
      "Cost 180 0.081990112851814\n",
      "Cost 181 0.08187063213823866\n",
      "Cost 182 0.08175215375201669\n",
      "Cost 183 0.0816346638156544\n",
      "Cost 184 0.08151814871749279\n",
      "Cost 185 0.08140259510522124\n",
      "Cost 186 0.08128798987958102\n",
      "Cost 187 0.08117432018825638\n",
      "Cost 188 0.08106157341994247\n",
      "Cost 189 0.08094973719858838\n",
      "Cost 190 0.08083879937780544\n",
      "Cost 191 0.08072874803543706\n",
      "Cost 192 0.08061957146828462\n",
      "Cost 193 0.08051125818698371\n",
      "Cost 194 0.0804037969110263\n",
      "Cost 195 0.08029717656392256\n",
      "Cost 196 0.08019138626849968\n",
      "Cost 197 0.08008641534233173\n",
      "Cost 198 0.07998225329329585\n",
      "Cost 199 0.07987888981525376\n",
      "Cost 200 0.07977631478384911\n",
      "Cost 201 0.07967451825242286\n",
      "Cost 202 0.07957349044803821\n",
      "Cost 203 0.07947322176761432\n",
      "Cost 204 0.07937370277416457\n"
     ]
    }
   ],
   "source": [
    "# getting the optimal coefficients through the gradient descent algorithm \n",
    "\n",
    "coeffs=run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictions for the test data and setting them to either 0 or 1\n",
    "\n",
    "Y_pred=predict(X_test, coeffs)\n",
    "\n",
    "for i in range(len(Y_pred)):\n",
    "    if Y_pred[i]<=0.5:\n",
    "        Y_pred[i]=0\n",
    "    else:\n",
    "        Y_pred[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the predictions in a csv file\n",
    "\n",
    "df=pd.DataFrame(np.array(Y_pred, dtype=int))\n",
    "\n",
    "df.to_csv(\"predictions.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "# printing the score for the gradient descent algorithm \n",
    "\n",
    "score=score(Y_pred, Y_test)\n",
    "\n",
    "print(score)"
   ]
  }
 ]
}